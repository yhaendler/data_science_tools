{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9849a9-ff1e-4f5a-941f-a286eacdd123",
   "metadata": {},
   "source": [
    "## Implementing a TF-IDF score calculator from scratch (more or less)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafc560-b808-4540-a3ea-7fa378c411d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39217946-855b-4dcd-b267-1d523f4bc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTextList(path):\n",
    "    import os\n",
    "    ## make a list of the files\n",
    "    filepath = [(path + os.listdir(path)[i]) for i in range(len(os.listdir(path)))]\n",
    "    ## create a list of all the texts extracted from each file\n",
    "    texts_list = []\n",
    "    for file in filepath:\n",
    "        with open(file) as f:\n",
    "            text = f.readlines()\n",
    "            texts_list.append(text[0])\n",
    "    return texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3c8c66-cd96-4829-a32d-79dfc8a5d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove special characters / marks / punctuation\n",
    "def removeMarks(word):\n",
    "    marks = '''!()[]{}<>;?@#$%:'\"\\,./^&*_0123456789'''\n",
    "    for char in word:\n",
    "        if char in marks:\n",
    "            word = word.replace(char, '')\n",
    "    return word\n",
    "\n",
    "## read and prepare file with stopwords, downloaded from an online list\n",
    "def getStopwords(filename='stopwords.txt'):\n",
    "    with open(filename) as f:\n",
    "        text = f.readlines()\n",
    "    stopwords = []\n",
    "    for word in text:\n",
    "        for char in ['\\n', '\\t']:\n",
    "            if char in word:\n",
    "                word = word.replace(char, '')\n",
    "        stopwords.append(word)\n",
    "    return stopwords\n",
    "\n",
    "## remove stopwords\n",
    "def removeStopwords(texts_list):\n",
    "    stopwords = getStopwords()\n",
    "    texts_without_stopwords = []\n",
    "    for text in texts_list:\n",
    "        single_text_without_stopwords = [word for word in text if word.lower() not in stopwords]\n",
    "        texts_without_stopwords.append(single_text_without_stopwords)\n",
    "    return texts_without_stopwords\n",
    "\n",
    "## remove marks and stopwords \n",
    "def removeMarksAndStopwords(texts_list, stem=True, stemmer='snowball'): \n",
    "    ## remove special marks and punctuation\n",
    "    cleaned_texts_list = [[removeMarks(word) for word in text.split()] for text in texts_list]\n",
    "    ## remove instances of empty strings like '' or \"\"\n",
    "    cleaned_texts_list = [[word for word in text if word] for text in cleaned_texts_list]\n",
    "    ## remove stopwords\n",
    "    cleaned_texts_list = removeStopwords(cleaned_texts_list)\n",
    "    \n",
    "    ## stem the words\n",
    "    if stem:\n",
    "        \n",
    "        ## set the stemmer type\n",
    "        if stemmer == 'lancaster':\n",
    "            from nltk.stem.lancaster import LancasterStemmer\n",
    "            st = LancasterStemmer()\n",
    "        elif stemmer == 'porter':\n",
    "            from nltk.stem.porter import PorterStemmer\n",
    "            st = PorterStemmer()\n",
    "        else:\n",
    "            from nltk.stem.snowball import SnowballStemmer\n",
    "            st = SnowballStemmer('english')\n",
    "        \n",
    "        ## stem each word in each text\n",
    "        cleaned_and_stemmed_texts_list = [[st.stem(word) for word in txt] for txt in cleaned_texts_list]\n",
    "        return cleaned_and_stemmed_texts_list\n",
    "        \n",
    "    return cleaned_texts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d59026-5349-41a6-97c5-7624d43df661",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF calculation\n",
    "def getTF(text):\n",
    "    tf_dict = {}\n",
    "    for word in text:\n",
    "        if word.lower() not in tf_dict.keys():\n",
    "            tf_dict[word.lower()] = 1\n",
    "        else:\n",
    "            tf_dict[word.lower()] += 1\n",
    "    for key,value in tf_dict.items():\n",
    "        tf_dict[key] = value / len(text)\n",
    "    return tf_dict\n",
    "\n",
    "## calculate TF scores\n",
    "def tfScores(cleaned_texts_list):\n",
    "    tf_list = [getTF(text) for text in cleaned_texts_list]\n",
    "    documents_list = [list(tf.keys()) for tf in tf_list]\n",
    "    return tf_list, documents_list\n",
    "\n",
    "## calculate IDF\n",
    "def getIDF(cleaned_texts_list):\n",
    "    import math\n",
    "    tf_list, documents_list = tfScores(cleaned_texts_list) \n",
    "    df_dict = {}\n",
    "    count = 0 \n",
    "    for term in tf_list:\n",
    "        for key in list(term.keys()):\n",
    "            for document in documents_list:\n",
    "                if key in document:\n",
    "                    count += 1\n",
    "            if key not in df_dict.keys():\n",
    "                df_dict[key] = math.log(len(documents_list) / count) + 1\n",
    "            count = 0\n",
    "    return tf_list, df_dict, documents_list\n",
    "\n",
    "## calculate TF-IDF\n",
    "def getTFIDF(cleaned_texts_list):\n",
    "    tf_list, df_dict, documents_list = getIDF(cleaned_texts_list)\n",
    "    tf_idf_dict = {}\n",
    "    for key,value in tf_list[0].items():\n",
    "        if key not in tf_idf_dict.keys():\n",
    "            tf_idf_dict[key] = value * df_dict[key]\n",
    "    tf_idf_dict_sorted = dict(sorted(tf_idf_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return tf_idf_dict_sorted, documents_list\n",
    "\n",
    "## TF-IDF summary table\n",
    "def tfidfSummary(cleaned_texts_list):\n",
    "    import pandas as pd\n",
    "    tf_idf_dict_sorted, documents_list = getTFIDF(cleaned_texts_list)\n",
    "    appearance_count = {}\n",
    "    for key,value in tf_idf_dict_sorted.items():\n",
    "        count = 0\n",
    "        for document in documents_list:\n",
    "            if key in document:\n",
    "                count += 1\n",
    "        appearance_count[key] = [value, count]\n",
    "    pd.set_option('display.max_rows', len(tf_idf_dict_sorted.keys()))\n",
    "    tf_idf_summary = pd.DataFrame(appearance_count, index=['TF-IDF_score','#_documents_word_appears']).T\n",
    "    return tf_idf_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c92be-9a4b-41e0-be3a-bd578e2249f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44d78a-d51c-406b-ac25-4cb830067aa5",
   "metadata": {},
   "source": [
    "The texts in the 'files' folder are taken from the Wikipedia English page for _Natural Language Processing_ (https://en.wikipedia.org/wiki/Natural_language_processing). They are used here only for the sake of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf2f6a7-6339-4364-833f-a9924f398956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of the texts to analyze\n",
    "## the txt files with the texts are stored in the 'files' folder\n",
    "texts = createTextList('./files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e45855-a69b-440d-9fed-fc93082e0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove special marks, punctuation and stopwords\n",
    "## optional: word stemming, chossing among 3 different NLTK stemmers\n",
    "cleaned_texts_list = removeMarksAndStopwords(texts,\n",
    "                                             stem=True # stem the words, use default stemmer (Snowball)\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ae310d-68c7-4ef0-ba82-e52e3369c067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF_score</th>\n",
       "      <th>#_documents_word_appears</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <td>0.145882</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>languag</th>\n",
       "      <td>0.129060</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comput</th>\n",
       "      <td>0.123632</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natur</th>\n",
       "      <td>0.113344</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>understand</th>\n",
       "      <td>0.084175</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>process</th>\n",
       "      <td>0.073104</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subfield</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scienc</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concern</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>program</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goal</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capabl</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contextu</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuanc</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technolog</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insight</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categor</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>organ</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challeng</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequent</th>\n",
       "      <td>0.059807</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linguist</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artifici</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intellig</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interact</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>particular</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>analyz</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extract</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recognit</th>\n",
       "      <td>0.048627</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>within</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accur</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inform</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contain</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>involv</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speech</th>\n",
       "      <td>0.042088</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>larg</th>\n",
       "      <td>0.037448</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>0.037448</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generat</th>\n",
       "      <td>0.037448</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>0.033849</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>includ</th>\n",
       "      <td>0.030908</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlp</th>\n",
       "      <td>0.026268</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.024368</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            TF-IDF_score  #_documents_word_appears\n",
       "document        0.145882                       2.0\n",
       "languag         0.129060                      13.0\n",
       "comput          0.123632                       6.0\n",
       "natur           0.113344                      10.0\n",
       "understand      0.084175                       3.0\n",
       "process         0.073104                       9.0\n",
       "subfield        0.059807                       1.0\n",
       "scienc          0.059807                       1.0\n",
       "concern         0.059807                       1.0\n",
       "program         0.059807                       1.0\n",
       "goal            0.059807                       1.0\n",
       "capabl          0.059807                       1.0\n",
       "contextu        0.059807                       1.0\n",
       "nuanc           0.059807                       1.0\n",
       "technolog       0.059807                       1.0\n",
       "insight         0.059807                       1.0\n",
       "well            0.059807                       1.0\n",
       "categor         0.059807                       1.0\n",
       "organ           0.059807                       1.0\n",
       "challeng        0.059807                       1.0\n",
       "frequent        0.059807                       1.0\n",
       "linguist        0.048627                       2.0\n",
       "artifici        0.048627                       2.0\n",
       "intellig        0.048627                       2.0\n",
       "interact        0.048627                       2.0\n",
       "particular      0.048627                       2.0\n",
       "analyz          0.048627                       2.0\n",
       "content         0.048627                       2.0\n",
       "extract         0.048627                       2.0\n",
       "recognit        0.048627                       2.0\n",
       "human           0.042088                       3.0\n",
       "within          0.042088                       3.0\n",
       "accur           0.042088                       3.0\n",
       "inform          0.042088                       3.0\n",
       "contain         0.042088                       3.0\n",
       "involv          0.042088                       3.0\n",
       "speech          0.042088                       3.0\n",
       "larg            0.037448                       4.0\n",
       "amount          0.037448                       4.0\n",
       "generat         0.037448                       4.0\n",
       "can             0.033849                       5.0\n",
       "includ          0.030908                       6.0\n",
       "nlp             0.026268                       8.0\n",
       "data            0.024368                       9.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate TF-IDF score\n",
    "tf_idf_summary = tfidfSummary(cleaned_texts_list)\n",
    "tf_idf_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529dd67-4c0e-494b-bce5-0b1bebf5a1d9",
   "metadata": {},
   "source": [
    "Next step to do: \n",
    "\n",
    "Implement an option to calculate a TF-IDF score for n-grams, rather than for single words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9d2a1-9148-4a11-9f55-140f94d051ca",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
